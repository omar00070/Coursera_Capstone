{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ibm_boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-99d74ee09358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mibm_boto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ibm_boto3'"
     ]
    }
   ],
   "source": [
    "#importing useful libraties\n",
    "import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "def __iter__(self): return 0\n",
    "\n",
    "#1. Importing Datasets from .csv files\n",
    "client_f3090439b2b44a619de5ccf8038a7a3e = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='oJ0x862uYpRKxwpU9FVS-PaIJWvbUNWYkkQ38LWmJRhL',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "#Accidents Dataset\n",
    "body = client_f3090439b2b44a619de5ccf8038a7a3e.get_object(Bucket='trafficaccidentseverity-donotdelete-pr-aynmftumcebntc',Key='DfTRoadSafety_Accidents_2019.csv')['Body']\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "dfa = pd.read_csv(body, index_col='Accident_Index')\n",
    "dfa.head()\n",
    "\n",
    "#Casualties Dataset\n",
    "body = client_f3090439b2b44a619de5ccf8038a7a3e.get_object(Bucket='trafficaccidentseverity-donotdelete-pr-aynmftumcebntc',Key='DfTRoadSafety_Casualties_2019.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "dfc = pd.read_csv(body, index_col='Accident_Index')\n",
    "dfc.head()\n",
    "\n",
    "#2. Cleaning Datasets\n",
    "#Merging Datasets\n",
    "df = dfa.join(dfc, on='Accident_Index')\n",
    "\n",
    "#Missing Values as Heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax = sns.heatmap(df.isna(), cbar=False)\n",
    "plt.xlabel('Variables', fontsize=15, labelpad=20, fontweight='bold')\n",
    "plt.ylabel('Data points',  labelpad=10, fontsize=15, fontweight='bold')\n",
    "plt.title('Heatmap of Missing Data', pad=20, fontsize=20, fontweight='bold')\n",
    "plt.xticks(fontsize=15);\n",
    "ax.yaxis.set_ticks(np.arange(0, 100, 100));\n",
    "\n",
    "#2. Dropping Missing Values in Dataset\n",
    "df = df.dropna()\n",
    "df.shape\n",
    "\n",
    "#Merging two close categories ('Serious' and 'Fatal') into one and creating classification dummy for Severity of accident (Serious = 1, Slight = 0)\n",
    "df['Severity_dummy'] = np.where(df['Accident_Severity']==3, 0, 1)\n",
    "\n",
    "#Identifying numeric features in dataset\n",
    "print('Continuous Variables become Numeric')\n",
    "print(df[['Sex_of_Casualty','Accident_Severity']].describe().transpose())\n",
    "\n",
    "#3. Exploratory data analysis of the dataset\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "ax = sns.heatmap(df.corr())#cbar=False)\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "fig = plt.figure(figsize = (20,16))\n",
    "fig.subplots_adjust(hspace = .30)\n",
    "\n",
    "plt.show()\n",
    "sns.set_style('darkgrid')\n",
    "fig = plt.figure(figsize = (20,16))\n",
    "fig.subplots_adjust(hspace = .30)\n",
    "\n",
    "#TARGET - Accident Severity (FATAL)\n",
    "df_fatal = df.loc[df['Accident_Severity'] == 1]\n",
    "df_fatal\n",
    " \n",
    "#Age (NON-FATAL)\n",
    "ax1 = fig.add_subplot(321)\n",
    "ax1.hist(df['Age_of_Casualty'], bins = 100, alpha = .50,edgecolor= 'black',color ='red')\n",
    "ax1.set_xlabel('Age_of_Casualty', labelpad=3, fontsize = 15)\n",
    "ax1.set_ylabel('# Accidents',fontsize = 15)\n",
    "ax1.set_title('Accidents by Age of Casualty',fontsize = 15)\n",
    "\n",
    "#Sex\n",
    "ax5 = fig.add_subplot(224)\n",
    "ax5.hist(df['Sex_of_Casualty'], bins = 6, alpha = .50,edgecolor= 'black',color ='red')\n",
    "ax5.set_xlabel('Sex_of_Casualty',fontsize = 15)\n",
    "ax5.set_ylabel('# Accidents',fontsize = 15)\n",
    "ax5.set_title('Accidents by Sex of Casualty',fontsize = 15)\n",
    "\n",
    "#Urban vs. Rural (FATAL)\n",
    "ax4 = fig.add_subplot(222)\n",
    "ax4.hist(df_fatal['Urban_or_Rural_Area'], bins = 4, alpha = .50,edgecolor= 'black',color ='red')\n",
    "ax4.set_xlabel('Urban_or_Rural_Area',fontsize = 15)\n",
    "ax4.set_ylabel('# Accidents',fontsize = 15)\n",
    "ax4.set_title('Accidents by Urban or Rural Area',fontsize = 15)\n",
    "\n",
    "#Urban vs Rural\n",
    "plt.show()\n",
    "df_fatal['Urban_or_Rural_Area'].value_counts()\n",
    "print(f'Share of fatal car accidents in Rural Area {(1727/(1727+723)*100)}%')\n",
    "print(f'Accidents in Rural Area happen {round((1727/723), 2)} times more than in Urban')\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "fig = plt.figure(figsize = (20,16))\n",
    "fig.subplots_adjust(hspace = .30)\n",
    "\n",
    "ax1 = fig.add_subplot(321)\n",
    "ax1.hist(df_fatal['Accident_Severity'], bins = 20, alpha = .50,edgecolor= 'black',color ='red')\n",
    "ax1.set_ylabel('# Accidents',fontsize = 15)\n",
    "ax1.set_title('Accident Severity',fontsize = 15)\n",
    "\n",
    "#Day of Week (FATAL)\n",
    "ax2 = fig.add_subplot(323)\n",
    "ax2.hist(df_fatal['Day_of_Week'], bins = 20, alpha = .50,edgecolor= 'black',color ='red')\n",
    "ax2.set_xlabel('Day_of_Week',fontsize = 15)\n",
    "ax2.set_xticklabels(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=90)\n",
    "ax2.set_ylabel('# Accidents',fontsize = 15)\n",
    "ax2.set_title('Accidents by Day of Week',fontsize = 15)\n",
    "\n",
    "#Weather Conditions (FATAL)\n",
    "ax3 = fig.add_subplot(325)\n",
    "ax3.hist(df_fatal['Weather_Conditions'], bins = 20, alpha = .50,edgecolor= 'black',color ='red')\n",
    "ax3.set_xlabel('Weather_Conditions',fontsize = 15)\n",
    "ax3.set_ylabel('# Accidents',fontsize = 15)\n",
    "ax3.set_title('Accidents by Weather_Conditions',fontsize = 15)\n",
    "\n",
    "#Road Surface Conditions (FATAL)\n",
    "ax4 = fig.add_subplot(222)\n",
    "ax4.hist(df_fatal['Road_Surface_Conditions'], bins = 20, alpha = .50,edgecolor= 'black',color ='red')\n",
    "ax4.set_xlabel('Road_Surface_Conditions',fontsize = 15)\n",
    "ax4.set_ylabel('# Accidents',fontsize = 15)\n",
    "ax4.set_title('Accidents by Road Surface Conditions',fontsize = 15)\n",
    "\n",
    "#Speed Limit (FATAL)\n",
    "ax5 = fig.add_subplot(224)\n",
    "ax5.hist(df_fatal['Speed_limit'], bins = 20, alpha = .50,edgecolor= 'black',color ='red')\n",
    "ax5.set_xlabel('Speed_limit',fontsize = 15)\n",
    "ax5.set_ylabel('# Accidents',fontsize = 15)\n",
    "ax5.set_title('Accidents by Speed Limit',fontsize = 15)\n",
    "\n",
    "plt.show()\n",
    "df.columns\n",
    "\n",
    "#TARGET - Accident Severity\n",
    "df['Accident_Severity'].value_counts()\n",
    "df.shape\n",
    "print(f'NULL accuraccy: {round((102837/132500*100), 2)}%')\n",
    "\n",
    "#Sex (FATAL)\n",
    "df_fatal['Sex_of_Casualty'].value_counts()\n",
    "target = 'Severity_dummy'\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "target = 'Severity_dummy'\n",
    "ax = df.groupby(target)['Number_of_Casualties'].count().plot(kind='bar', color='red', alpha=0.3);\n",
    "plt.grid(axis='y', alpha=0.5, linestyle='--', linewidth=0.3)\n",
    "plt.xlabel(target, fontsize=18, fontweight='bold')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Number of accidents', fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(f'Accidents by Severity', pad=15, fontsize=16, fontweight='bold');\n",
    "\n",
    "df.columns\n",
    "df.head()\n",
    "df.shape\n",
    "\n",
    "labels = ['Number_of_Vehicles', 'Number_of_Casualties', 'Day_of_Week', '1st_Road_Class', '1st_Road_Number', \n",
    "                    'Road_Type', 'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class', \n",
    "                    '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities', \n",
    "                    'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions', 'Special_Conditions_at_Site', \n",
    "                    'Carriageway_Hazards', 'Urban_or_Rural_Area','Vehicle_Reference', 'Casualty_Reference', \n",
    "                    'Casualty_Class', 'Sex_of_Casualty', 'Age_of_Casualty', 'Age_Band_of_Casualty', \n",
    "                    'Pedestrian_Location', 'Pedestrian_Movement', 'Car_Passenger', 'Bus_or_Coach_Passenger',\n",
    "                    'Pedestrian_Road_Maintenance_Worker', 'Casualty_Type', 'Casualty_Home_Area_Type']\n",
    "\n",
    "#Principal Component Analysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features = ['Number_of_Vehicles', 'Accident_Severity', 'Number_of_Casualties', 'Day_of_Week', '1st_Road_Class', '1st_Road_Number', \n",
    "                    'Road_Type', 'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class', \n",
    "                    '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities', \n",
    "                    'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions', 'Special_Conditions_at_Site', \n",
    "                    'Carriageway_Hazards', 'Urban_or_Rural_Area','Vehicle_Reference', 'Casualty_Reference', \n",
    "                    'Casualty_Class', 'Sex_of_Casualty', 'Age_of_Casualty', 'Age_Band_of_Casualty', \n",
    "                    'Pedestrian_Location', 'Pedestrian_Movement', 'Car_Passenger', 'Bus_or_Coach_Passenger',\n",
    "                    'Pedestrian_Road_Maintenance_Worker', 'Casualty_Type', 'Casualty_Home_Area_Type']\n",
    "\n",
    "x = df[features]\n",
    "y = df[target]\n",
    "\n",
    "\n",
    "len(features)\n",
    "\n",
    "#4. Data Standardisation and Sampling\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "#Principal Components\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2',\n",
    "                                                                 'principal component 3','principal component 4',\n",
    "                                                                 'principal component 5',\n",
    "                                                                 'principal component 6',\n",
    "                                                                 'principal component 7',\n",
    "                                                                 'principal component 8',\n",
    "                                                                 'principal component 9',\n",
    "                                                                 'principal component 10',\n",
    "                                                                 'principal component 11',\n",
    "                                                                 'principal component 12',\n",
    "                                                                 'principal component 13',\n",
    "                                                                 'principal component 14',\n",
    "                                                                 'principal component 15',\n",
    "                                                                 'principal component 16',\n",
    "                                                                 'principal component 17',\n",
    "                                                                 'principal component 18',\n",
    "                                                                 'principal component 19',\n",
    "                                                                 'principal component 20',\n",
    "                                                                 'principal component 21',\n",
    "                                                                 'principal component 22',\n",
    "                                                                 'principal component 23',\n",
    "                                                                 'principal component 24',\n",
    "                                                                 'principal component 25',\n",
    "                                                                 'principal component 26',\n",
    "                                                                 'principal component 27',\n",
    "                                                                 'principal component 28',\n",
    "                                                                 'principal component 29',\n",
    "                                                                 'principal component 30',\n",
    "                                                                 'principal component 31',\n",
    "                                                                 'principal component 32',\n",
    "                                                                 'principal component 33'])\n",
    "\n",
    "\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "#Baseline/XGBoost model  model (1st XGBoost)\n",
    "X = df[labels]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "#Y Test\n",
    "y_test.shape\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print('Confusion matrix for XGBoost model:\\n', conf_mat)\n",
    "\n",
    "labels1 = ['Slight', 'Serious']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels1)\n",
    "ax.set_yticklabels([''] + labels1)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()\n",
    "\n",
    "#XGBoost perfomance with just one variable\n",
    "model1 = XGBClassifier()\n",
    "model1.fit(X_train[['Day_of_Week']], y_train)\n",
    "y_pred1 = model1.predict(X_test[['Day_of_Week']])\n",
    "\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy1 * 100.0))\n",
    "\n",
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "Xl = df[labels]\n",
    "yl = df[target]\n",
    "X_trainl, X_testl, y_trainl, y_testl = train_test_split(Xl, yl, test_size=0.2, random_state=1)\n",
    "logreg = LogisticRegression().fit(X_trainl, y_trainl)\n",
    "y_predl = logreg.predict(X_testl)\n",
    "accuracy2 = logreg.score(Xl, yl, sample_weight=None)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy2 * 100.0))\n",
    "\n",
    "conf_matl = confusion_matrix(y_true=y_testl, y_pred=y_predl)\n",
    "print('Confusion matrix for LOGISTIC regression:\\n', conf_matl)\n",
    "\n",
    "labelsl = ['Slight', 'Severe']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labelsl)\n",
    "ax.set_yticklabels([''] + labelsl)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()\n",
    "\n",
    "from sklearn import metrics\n",
    "THRESHOLD = 0.0\n",
    "preds = np.where(logreg.predict(X_testl) > THRESHOLD, 1, 0)\n",
    "\n",
    "pd.DataFrame(data=[metrics.accuracy_score(y_testl, preds), metrics.recall_score(y_testl, preds),\n",
    "                   metrics.precision_score(y_testl, preds), metrics.f1_score(y_testl, preds)], \n",
    "             index=[\"accuracy\", \"recall\", \"precision\", \"F1\"])\n",
    "\n",
    "! pip install imbalanced-learn\n",
    "import imblearn\n",
    "\n",
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "plot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')  \n",
    "\n",
    "#Random Under Sampling (RUS)\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X = df[labels]\n",
    "y = df[target]\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_rus, y_rus = rus.fit_sample(X, y)\n",
    "\n",
    "print(X_rus.shape)\n",
    "print(y_rus.shape)\n",
    "\n",
    "#XGBoost Predictions RUS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print('Confusion matrix for XGBoost modelRUS:\\n', conf_mat)\n",
    "\n",
    "labels1 = ['Slight', 'Severe']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels1)\n",
    "ax.set_yticklabels([''] + labels1)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()\n",
    "\n",
    "THRESHOLD = 0\n",
    "preds = np.where(model.predict(X_test) > THRESHOLD, 1, 0)\n",
    "\n",
    "pd.DataFrame(data=[metrics.accuracy_score(y_test, preds), metrics.recall_score(y_test, preds),\n",
    "                   metrics.precision_score(y_test, preds), metrics.f1_score(y_test, preds)], \n",
    "             index=[\"accuracy\", \"recall\", \"precision\", \"F1\"])\n",
    "\n",
    "#Random Over Sampling (ROS)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(X, y)\n",
    "\n",
    "print(X_ros.shape[0] - X.shape[0], 'new random picked points')\n",
    "\n",
    "print(X_ros.shape)\n",
    "print(y_ros.shape)\n",
    "\n",
    "#XGBoost predictions ROS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print('Confusion matrix for XGBoost model ROS:\\n', conf_mat)\n",
    "\n",
    "labels1 = ['Slight', 'Severe']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels1)\n",
    "ax.set_yticklabels([''] + labels1)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()\n",
    "\n",
    "#Confusion Matrix Slicing\n",
    "confusion = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "THRESHOLD = 0.0\n",
    "preds = np.where(model.predict(X_test) > THRESHOLD, 1, 0)\n",
    "\n",
    "pd.DataFrame(data=[metrics.accuracy_score(y_test, preds), metrics.recall_score(y_test, preds),\n",
    "                   metrics.precision_score(y_test, preds), metrics.f1_score(y_test, preds)], \n",
    "             index=[\"accuracy\", \"recall\", \"precision\", \"F1\"])\n",
    "\n",
    "\n",
    "#5. Statistical Analysis\n",
    "#Accuracy\n",
    "print(round((TP + TN) / float(TP + TN + FP + FN), 4)*100)\n",
    "\n",
    "#Precision\n",
    "from sklearn import metrics\n",
    "print((TP / float(TP + FP))*100)\n",
    "print(metrics.precision_score(y_test, y_pred)*100)\n",
    "\n",
    "#Sensitivity\n",
    "print(TP / float(TP + FN))\n",
    "print(metrics.recall_score(y_test, y_pred))\n",
    "print(TN / float(TN + FP))\n",
    "\n",
    "#F1 score\n",
    "2*(metrics.precision_score(y_test, y_pred)*metrics.recall_score(y_test, y_pred))/(metrics.precision_score(y_test, y_pred)+metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "#Decision Tree Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#Fitting the DecisionTreeClassifier\n",
    "X = df[labels]\n",
    "y = df[target]\n",
    "X_ros, y_ros = ros.fit_sample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.2, random_state=1)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(criterion='gini', max_depth=4) \n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "#Feature importance\n",
    "tree_clf.feature_importances_\n",
    "\n",
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(tree_clf)\n",
    "\n",
    "#Test Set Predictions\n",
    "pred = tree_clf.predict(X_test)\n",
    "\n",
    "#Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "print(\"Testing Accuracy for Decision Tree Classifier: {:.4}%\".format(accuracy_score(y_test, pred) * 100))\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "#Regression Tree visualisation\n",
    "dot_data = StringIO()\n",
    "export_graphviz(tree_clf, out_file=dot_data,  \n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())\n",
    "\n",
    "X = df[labels]\n",
    "y = df[target]\n",
    "X_ros, y_ros = ros.fit_sample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.2, random_state=1)\n",
    "\n",
    "#Fitting RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth= 5)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "#Test Set Predictions\n",
    "predF = forest.predict(X_test)\n",
    "\n",
    "#Confusion matrix and Classification Report\n",
    "print(confusion_matrix(y_test, predF))\n",
    "print(classification_report(y_test, predF))\n",
    "\n",
    "#Training Accuracy Score\n",
    "forest.score(X_train, y_train)\n",
    "\n",
    "#Test Accuracy Score\n",
    "forest.score(X_test, y_test)\n",
    "\n",
    "#Feature importance\n",
    "plot_feature_importances(forest)\n",
    "y_ros.value_counts()\n",
    "\n",
    "#Under-sampling: Tomek links\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "X = df[labels]\n",
    "y = df[target]\n",
    "\n",
    "tl = TomekLinks(sampling_strategy='majority')\n",
    "X_tl, y_tl = tl.fit_sample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tl, y_tl, test_size=0.2, random_state=1)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth= 5)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "#Test Set Predictions\n",
    "predF = forest.predict(X_test)\n",
    "\n",
    "#Confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, predF))\n",
    "print(classification_report(y_test, predF))\n",
    "\n",
    "#Over-sampling: SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = df[labels]\n",
    "y = df[target]\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto')\n",
    "X_sm, y_sm = smote.fit_sample(X, y)\n",
    "\n",
    "X_sm.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=1)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth= 5)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "#Test Set Predictions\n",
    "predF = forest.predict(X_test)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, predF))\n",
    "print(classification_report(y_test, predF))\n",
    "\n",
    "#XGBoost SMOTE\n",
    "X = df[labels]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=1)\n",
    "\n",
    "model_xg_sm = XGBClassifier()\n",
    "model_xg_sm.fit(X_train, y_train)\n",
    "y_pred = model_xg_sm.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print('Confusion matrix for XGBoost model ROS:\\n', conf_mat)\n",
    "\n",
    "labels1 = ['Slight', 'Severe']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels1)\n",
    "ax.set_yticklabels([''] + labels1)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()\n",
    "\n",
    "# Test set predictions\n",
    "predXG = model_xg_sm.predict(X_test)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, predXG))\n",
    "print(classification_report(y_test, predXG))\n",
    "\n",
    "plot_feature_importances(model_xg_sm)\n",
    "\n",
    "df['Vehicle_Reference'].value_counts()\n",
    "\n",
    "#Decision Tree Classifier SMOTE\n",
    "X = df[labels]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=1)\n",
    "\n",
    "tree_clf_sm = DecisionTreeClassifier(criterion='gini', max_depth=4) \n",
    "tree_clf_sm.fit(X_train, y_train)\n",
    "\n",
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(tree_clf_sm)\n",
    "\n",
    "#Test Set Predictions\n",
    "pred = tree_clf_sm.predict(X_test)\n",
    "\n",
    "#Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "#Random Forest SMOTE\n",
    "X = df[labels]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=1)\n",
    "\n",
    "forest_sm = RandomForestClassifier(n_estimators=100, max_depth= 5)\n",
    "forest_sm.fit(X_train, y_train)\n",
    "y_pred = forest_sm.predict(X_test)\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print('Confusion matrix for XGBoost model ROS:\\n', conf_mat)\n",
    "\n",
    "labels1 = ['Slight', 'Severe']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels1)\n",
    "ax.set_yticklabels([''] + labels1)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()\n",
    "\n",
    "#Test Set Predictions\n",
    "predRF = forest_sm.predict(X_test)\n",
    "\n",
    "#Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y_test, predRF))\n",
    "print(classification_report(y_test, predRF))\n",
    "plot_feature_importances(forest_sm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ibm-cos-sdk==1.0.2\n",
      "  Downloading ibm-cos-sdk-1.0.2.tar.gz (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ibm-cos-sdk-core==1.0.*,>=1.0.0\n",
      "  Downloading ibm-cos-sdk-core-1.0.2.tar.gz (965 kB)\n",
      "\u001b[K     |████████████████████████████████| 965 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==1.0.*,>=1.0.0\n",
      "  Downloading ibm_cos_sdk_s3transfer-1.0.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 2.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting docutils>=0.10\n",
      "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "\u001b[K     |████████████████████████████████| 548 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/omars/.local/lib/python3.8/site-packages (from ibm-cos-sdk-core==1.0.*,>=1.0.0->ibm-cos-sdk==1.0.2) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/omars/.local/lib/python3.8/site-packages (from ibm-cos-sdk-core==1.0.*,>=1.0.0->ibm-cos-sdk==1.0.2) (2.8.1)\n",
      "Collecting requests<=2.18.4,>=2.12.0\n",
      "  Downloading requests-2.18.4-py2.py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/omars/.local/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==1.0.*,>=1.0.0->ibm-cos-sdk==1.0.2) (1.13.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<=2.18.4,>=2.12.0->ibm-cos-sdk-core==1.0.*,>=1.0.0->ibm-cos-sdk==1.0.2) (3.0.4)\n",
      "Collecting idna<2.7,>=2.5\n",
      "  Downloading idna-2.6-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 1.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting urllib3<1.23,>=1.21.1\n",
      "  Downloading urllib3-1.22-py2.py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/omars/.local/lib/python3.8/site-packages (from requests<=2.18.4,>=2.12.0->ibm-cos-sdk-core==1.0.*,>=1.0.0->ibm-cos-sdk==1.0.2) (2020.6.20)\n",
      "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core\n",
      "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-1.0.2-py2.py3-none-any.whl size=66718 sha256=dbfa1fd7c9a8a72c588ca5ec4763a8ccfc98a5148d2a3c27e706ebea1672007c\n",
      "  Stored in directory: /home/omars/.cache/pip/wheels/6c/89/24/6908c601842dfa7b8dbfb93d4c462343d03ce40bf7ccbf6719\n",
      "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-1.0.2-py2.py3-none-any.whl size=678149 sha256=a4b0f189c29ea2da526f4101d213e0ae9c810d9342a68c50bbcb4b38acb58504\n",
      "  Stored in directory: /home/omars/.cache/pip/wheels/24/91/d9/9721d9d35001846d495b53d60d0c7f336d7d0d19cee820a4bd\n",
      "Successfully built ibm-cos-sdk ibm-cos-sdk-core\n",
      "\u001b[31mERROR: spotipy 2.14.0 has requirement requests>=2.20.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pyppeteer 0.2.2 has requirement urllib3<2.0.0,>=1.25.8, but you'll have urllib3 1.22 which is incompatible.\u001b[0m\n",
      "Installing collected packages: docutils, idna, urllib3, requests, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.10\n",
      "    Uninstalling urllib3-1.25.10:\n",
      "      Successfully uninstalled urllib3-1.25.10\n",
      "Successfully installed docutils-0.16 ibm-cos-sdk-1.0.2 ibm-cos-sdk-core-1.0.2 ibm-cos-sdk-s3transfer-1.0.1 idna-2.6 requests-2.18.4 urllib3-1.22\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ibm-cos-sdk==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitf40e50bd62ba49f590f614c93fb9ed42"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
